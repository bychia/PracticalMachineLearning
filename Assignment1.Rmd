---
title: "Data Science: Machine Learning Project"
author: "BY.Chia"
date: "March 10th, 2015"
output: html_document
---

### Executive Summary
In the training set, there is the "classe" variable. And we are going to use the other variables in the same dataset to make prediction in the testing data set. 

We are going to use rpart and random forest machine learning methods and choose the best method to make the prediction. We favour the random forest method because it yields an Accuracy of 99.81% while the rpart only gives 59.83%.  

##### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 


##### Data 

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


```{r loadLibrary, error=FALSE, warning=FALSE, results='hide'}
#Load all necessary Libraries
library(caret)
library(rattle)
library(rpart)
library(randomForest)
```

### Load Data 

We first load the train and test data from the urls provided.

```{r loadData}
#Set seed for research reproducibility
set.seed(12323)

#Getting the train and test files URL:
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

directory <- getwd()
download.file(trainUrl, paste(directory, "/pml-training.csv", sep=""), method="curl")
download.file(testUrl, paste(directory, "/pml-testing.csv", sep=""), method="curl")

training <- read.csv("pml-training.csv", header=TRUE, sep=",", na.strings=c("", "NA", "NULL"))
testing <- read.csv("pml-testing.csv", header=TRUE, sep=",", na.strings=c("", "NA", "NULL"))
```


### Exploratory Analysis

We will explore training and testing datasets and find out any missing values in them.

```{r exploratoryData}
dim(training)
dim(testing)
totalNAvaluesByColumnsInTrain <- colSums(!is.na(training))
totalNAvaluesByColumnsInTest <- colSums(!is.na(testing))
```

### Data Cleaning

Since all columns contain some missing values from our earlier exploratory analysis, we can't remove all of them. We will need to keep some as our predictors. Hence, we will only remove columns with more than 40% of NA values in them.

```{r cleaningData}
# If it has more than 40% of NA values, we will remove it.
goodTrainData <- totalNAvaluesByColumnsInTrain>0.6*(nrow(training))
goodTrainDataName <- names(goodTrainData[goodTrainData==TRUE])
goodTestData <- totalNAvaluesByColumnsInTest>0.6*(nrow(testing))
goodTestDataName <- names(goodTestData[goodTestData==TRUE])

# Keep only the good columns in both train and test sets.
training <- training[goodTrainData]
testing <- testing[goodTrainData]
```

### Data Partitioning

We'll partition the Training data set into two data sets, 60% for myTraining, 40% for myTesting. We'll keep the Testing data set as a validation set.

```{r partitionData}
# Partition data
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain,]
myTesting <- training[-inTrain,]

# To make all the testing and validation consistent with myTraining dataset
col1 <- colnames(myTraining)
# Remove classe column as it does not exist in our validation dataset
col2 <- colnames(myTraining[, -60]) 
myTesting <- myTesting[col1]
testing <- testing[col2]
```


### Data Processing for prediction modelling

To ensure that all the columns in myTraining dataset are of the same type with all the columns in testing dataset, we will use `rbind` to coerce the data into consistent type.

```{r processData}
# Take 1st row of myTraining and combine with testing. This coerce the data into the same type as myTraining dataset
myTesting <- rbind(myTraining[1,] , myTesting) 
# Now removing the first row of the data. Nothing in testing dataset has changed.
myTesting <- myTesting[-1,]

testing <- rbind(myTraining[1, -60] , testing) 
# Now removing the first row of the data. Nothing in testing dataset has changed.
testing <- testing[-1,]

# Finally, removing all first column variable `x` as it cannot be used as a predictor.
myTraining <- subset(myTraining, select= -X)
myTesting <- subset(myTesting, select= -X)
testing <- subset(testing, select= -X)
```

### Using rpart algorithm for prediction

```{r prediction1}
modFitRPart <- train(classe ~ ., data=myTraining,  method="rpart")
# Prediction on myTesting dataset
predictionRPart <- predict(modFitRPart, myTesting)
# ConfusionMatrix
CMRPart <- confusionMatrix(predictionRPart, myTesting$classe)
# Accuracy
CMRPart$overall[1]
```

```{r predictionSampleError, echo=FALSE}
CMRPart[3]
```

The out sample error rate from rpart model is: `r 1-CMRPart$overall[1]`.
This out sample error is quite high. And we will not use this predictive model. 


### Using random forest algorithm for prediction

```{r prediction2}
## Using ML algorithms for prediction: Random Forests
modFitRF <- randomForest(classe ~. , data=myTraining)
## Prediction on myTesting dataset
predictionsRF <- predict(modFitRF, myTesting, type = "class")
## ConfusionMatrix
CMRF <- confusionMatrix(predictionsRF, myTesting$classe)
# Accuracy
CMRF$overall[1]
```

```{r prediction2SampleError, echo=FALSE}
CMRF[3]
```

The out sample error rate from rpart model is: `r 1-CMRF$overall[1]`.
This out sample error is very low. And we will use random forest as our predictive model. 

### Generating Files to submit as answers for the Assignment:

From the different model results (see Annex), the random forest yields a higher accuracy of `r CMRF$overall[1]`, we will use the model to predict for the testing dataset.

```{r answers}
#Function to generate files with predictions to submit for assignment:
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

predictionsRF2 <- predict(modFitRF, testing, type = "class")
pml_write_files(predictionsRF2)
answers <- as.character(predictionsRF2)
answers
```

# Annex
### RPart Model Results
```{r echo=FALSE}
CMRPart
```

### RPart Figure Plot
```{r rpartPlot}
fancyRpartPlot(modFitRPart$finalModel)
```

### Random Forest Model Results
```{r echo=FALSE}
CMRF
```

### Random Forest Table
```{r randomForestTable, results='hide'}
#result hidden as it is too long
randomForestTable <- getTree(modFitRF, 1, labelVar=TRUE) 
```

